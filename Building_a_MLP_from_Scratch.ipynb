{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FnKsTCiq2dJQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  return np.mean( (y_true - y_pred)**2 )\n",
        "\n",
        "def forward(X, weights_input_hidden1, weights_hidden1_hidden2, weights_hidden2_output,\n",
        "            bias_input_hidden1, bias_hidden1_hidden2, bias_hidden2_output):\n",
        "    # 第一層\n",
        "    hidden1_output = sigmoid(np.dot(X, weights_input_hidden1) + bias_input_hidden1)\n",
        "\n",
        "    # 第二層\n",
        "    hidden2_output = sigmoid(np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden1_hidden2)\n",
        "\n",
        "    # 輸出層\n",
        "    output = sigmoid(np.dot(hidden2_output, weights_hidden2_output) + bias_hidden2_output)\n",
        "\n",
        "    return output, hidden1_output, hidden2_output\n",
        "\n",
        "\n",
        "def backward(X, y_true, output, hidden1_output, hidden2_output, weights_input_hidden1,\n",
        "             weights_hidden1_hidden2, weights_hidden2_output, bias_input_hidden1,\n",
        "             bias_hidden1_hidden2, bias_hidden2_output, learning_rate):\n",
        "    # 計算輸出層的梯度\n",
        "    error = y_true - output\n",
        "    delta_output = error * sigmoid_derivative(output)\n",
        "\n",
        "    # 第二層\n",
        "    # error_hidden2: 第二層的誤差 delta_hidden2: 第二層的梯度\n",
        "    error_hidden2 = delta_output.dot(weights_hidden2_output.T)\n",
        "    delta_hidden2 = error_hidden2 * sigmoid_derivative(hidden2_output)\n",
        "\n",
        "    # 第一層\n",
        "    # error_hidden1: 第一層的誤差 delta_hidden1: 第一層的梯度\n",
        "    error_hidden1 = delta_hidden2.dot(weights_hidden1_hidden2.T)\n",
        "    delta_hidden1 = error_hidden1 * sigmoid_derivative(hidden1_output)\n",
        "\n",
        "    # 更新權重: W' = W + LR * 權重的梯度\n",
        "    weights_hidden2_output += hidden2_output.T.dot(delta_output) * learning_rate\n",
        "    weights_hidden1_hidden2 += hidden1_output.T.dot(delta_hidden2) * learning_rate\n",
        "    weights_input_hidden1 += X.T.dot(delta_hidden1) * learning_rate\n",
        "\n",
        "    # 更新偏差: b' = b + LR * 偏差的梯度\n",
        "    bias_hidden2_output += np.sum(delta_output, axis=0) * learning_rate\n",
        "    bias_hidden1_hidden2 += np.sum(delta_hidden2, axis=0) * learning_rate\n",
        "    bias_input_hidden1 += np.sum(delta_hidden1, axis=0) * learning_rate\n",
        "\n",
        "def initialize_parameters(input_size, hidden1_size, hidden2_size, output_size):\n",
        "    # 初始化權重\n",
        "    weights_input_hidden1 = np.random.randn(input_size, hidden1_size)\n",
        "    weights_hidden1_hidden2 = np.random.randn(hidden1_size, hidden2_size)\n",
        "    weights_hidden2_output = np.random.randn(hidden2_size, output_size)\n",
        "\n",
        "    # 初始化偏差\n",
        "    bias_input_hidden1 = np.zeros((1, hidden1_size))\n",
        "    bias_hidden1_hidden2 = np.zeros((1, hidden2_size))\n",
        "    bias_hidden2_output = np.zeros((1, output_size))\n",
        "\n",
        "    return weights_input_hidden1, weights_hidden1_hidden2, weights_hidden2_output, \\\n",
        "           bias_input_hidden1, bias_hidden1_hidden2, bias_hidden2_output\n",
        "\n",
        "\n",
        "def train(X, y_true, epochs, learning_rate, input_size, hidden1_size, hidden2_size, output_size):\n",
        "    # 初始化權重和偏差\n",
        "    weights_input_hidden1, weights_hidden1_hidden2, weights_hidden2_output, \\\n",
        "    bias_input_hidden1, bias_hidden1_hidden2, bias_hidden2_output = \\\n",
        "        initialize_parameters(input_size, hidden1_size, hidden2_size, output_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 前向傳播計算輸出和誤差\n",
        "        output, hidden1_output, hidden2_output = forward(X, weights_input_hidden1, weights_hidden1_hidden2,\n",
        "                                      weights_hidden2_output, bias_input_hidden1,\n",
        "                                      bias_hidden1_hidden2, bias_hidden2_output)\n",
        "        loss = mse_loss(y_true, output)\n",
        "\n",
        "        # 反向傳播更新參數\n",
        "        backward(X, y_true, output, hidden1_output, hidden2_output, weights_input_hidden1,\n",
        "                 weights_hidden1_hidden2, weights_hidden2_output, bias_input_hidden1,\n",
        "                 bias_hidden1_hidden2, bias_hidden2_output, learning_rate)\n",
        "\n",
        "        # 每1000次迭代打印一次損失\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "\n",
        "    return weights_input_hidden1, weights_hidden1_hidden2, weights_hidden2_output, \\\n",
        "           bias_input_hidden1, bias_hidden1_hidden2, bias_hidden2_output\n"
      ],
      "metadata": {
        "id": "YS9yNKXQ-DAP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_true = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "input_size = X.shape[1]\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "output_size = y_true.shape[1]\n",
        "\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "trained_params = train(X, y_true, epochs, learning_rate, input_size, hidden1_size, hidden2_size, output_size)\n"
      ],
      "metadata": {
        "id": "XWNgu6eRH0E1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30023a01-659c-4e9e-e88d-b613ee0a7b99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2599325613535397\n",
            "Epoch 1000, Loss: 0.24006020838744463\n",
            "Epoch 2000, Loss: 0.11960399889780675\n",
            "Epoch 3000, Loss: 0.01235012486780169\n",
            "Epoch 4000, Loss: 0.004335372905750736\n",
            "Epoch 5000, Loss: 0.002417037313370791\n",
            "Epoch 6000, Loss: 0.0016245719381712295\n",
            "Epoch 7000, Loss: 0.0012049753194590634\n",
            "Epoch 8000, Loss: 0.0009492663680188012\n",
            "Epoch 9000, Loss: 0.0007787095059457619\n"
          ]
        }
      ]
    }
  ]
}